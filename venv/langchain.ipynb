{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'%pip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!%pip install --upgrade --quiet  langchain-google-genai pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Provide your Google API Key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Lewis Hamilton**\n",
      "\n",
      "**Personal Information:**\n",
      "\n",
      "* **Full Name:** Lewis Carl Davidson Hamilton MBE\n",
      "* **Date of Birth:** January 7, 1985\n",
      "* **Birth Place:** Stevenage, Hertfordshire, England\n",
      "* **Nationality:** British\n",
      "\n",
      "**Career:**\n",
      "\n",
      "* **Formula One Driver:**\n",
      "\n",
      "    * **Team:** Mercedes-AMG Petronas Formula One Team\n",
      "    * **Debut:** 2007 Australian Grand Prix\n",
      "    * **World Championships:** 7 (2008, 2014, 2015, 2017, 2018, 2019, 2020)\n",
      "\n",
      "* **Other Notable Achievements:**\n",
      "\n",
      "    * **Most Grand Prix wins:** 103\n",
      "    * **Most pole positions:** 103\n",
      "    * **Most podium finishes:** 191\n",
      "    * **Longest consecutive podium finishes:** 24 (2014-2016)\n",
      "\n",
      "**Personal Life:**\n",
      "\n",
      "* **Parents:** Anthony Hamilton (father), Carmen Larbalestier (mother)\n",
      "* **Siblings:** Nicolas Hamilton (brother)\n",
      "* **Hobbies:** Music, fashion, fitness, travel\n",
      "\n",
      "**Accolades and Honors:**\n",
      "\n",
      "* **Member of the Order of the British Empire (MBE):** 2008\n",
      "* **Knighthood:** 2021\n",
      "* **BBC Sports Personality of the Year:** 2014, 2015, 2017, 2019, 2020\n",
      "* **Laureus World Sportsman of the Year:** 2020\n",
      "\n",
      "**Legacy:**\n",
      "\n",
      "Lewis Hamilton is widely regarded as one of the greatest Formula One drivers of all time. His exceptional driving skills, unwavering determination, and record-breaking achievements have made him an icon in the sport.\n"
     ]
    }
   ],
   "source": [
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
    "result = llm.invoke(\"who is Lewis Hamilton\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kyoto\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain import HuggingFaceHub\n",
    "\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\"\n",
    "\n",
    "# Initialize HuggingFaceHub\n",
    "hugging_face_hub = HuggingFaceHub(\n",
    "    repo_id=\"google/flan-t5-large\", \n",
    "    model_kwargs={\"temperature\": 0, \"max_length\": 64}\n",
    ")\n",
    "output = hugging_face_hub.predict(\"Can you tell me the capital of Japan?\")\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cricket i 'm a cricketer i 'm a cricketer i 'm a cricketer i 'm a cricketer i 'm a cricketer i 'm a cricketer i 'm a cricket\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain import HuggingFaceHub\n",
    "\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\"\n",
    "\n",
    "# Initialize HuggingFaceHub\n",
    "hugging_face_hub = HuggingFaceHub(\n",
    "    repo_id=\"google/flan-t5-large\", \n",
    "    model_kwargs={\"temperature\": 0, \"max_length\": 64}\n",
    ")\n",
    "output = hugging_face_hub.predict(\"generate me a 5 lines of song on cricket \")\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_20372\\3297772645.py:1: LangChainDeprecationWarning: The method `BaseChatModel.predict` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  output = llm.predict(\"generate me a 5 lines of song on cricket \")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On the pitch, a battle unfolds,\n",
      "Bat and ball, a tale yet untold.\n",
      "Bowlers spin, fielders dash,\n",
      "As the tension and excitement clash.\n",
      "Cricket's symphony, a thrilling tune,\n",
      "Where skill and strategy intertwine, under the sun.\n"
     ]
    }
   ],
   "source": [
    "output = llm.predict(\"generate me a 5 lines of song on cricket \")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROMPT TEMPLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me the capital of India.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Create a prompt template\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=['country'],\n",
    "    template=\"Tell me the capital of {country}.\"\n",
    ")\n",
    "\n",
    "# Format the prompt with a specific country\n",
    "formatted_prompt = prompt_template.format(country='India')\n",
    "print(formatted_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The color of the sun is actually white, but it appears yellow to us because of the way our atmosphere scatters the light.\\n\\nWhen sunlight enters Earth's atmosphere, it encounters tiny molecules of gases such as nitrogen and oxygen. These molecules scatter the shorter, blue wavelengths of light more than the longer, red wavelengths. This is known as Rayleigh scattering, named after the British physicist Lord Rayleigh, who first described the phenomenon in the late 19th century.\\n\\nAs a result of this scattering, the blue light is dispersed in all directions and reaches our eyes from all parts of the sky. The red and yellow light, which is not scattered as much, continues to travel in a more direct path to our eyes, giving the sun its yellowish hue.\\n\\nIf we were to view the sun from space, outside of our atmosphere, it would appear as a brilliant white star, emitting light of all wavelengths equally in all directions. But because of the scattering effect, the sun appears yellow to us on Earth.\\n\\nIt's worth noting that the color of the sun can appear to vary slightly depending on the time of day and the amount of dust and water vapor in the atmosphere. For example, during sunrise and sunset, the sun can take on hues of orange and red due to the increased scattering of light by atmospheric particles. But in general, the sun appears yellow to us because of the scattering of light by our atmosphere.\""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#while using prompt template we must use chain if we doesnt we get error :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "london\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=['country'],\n",
    "    template=\"Tell me the capital of {country}.\"\n",
    ")\n",
    "# Use LLMChain with the initialized model\n",
    "chain = LLMChain(llm=hugging_face_hub, prompt=prompt_template)\n",
    "# Run the chain with input\n",
    "output = chain.run({'country': 'UK'})\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.0.284)\n",
      "Collecting genai\n",
      "  Downloading genai-2.1.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain) (2.0.36)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain) (3.10.10)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain) (0.5.14)\n",
      "Collecting langsmith<0.1.0,>=0.0.21 (from langchain)\n",
      "  Downloading langsmith-0.0.92-py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain) (2.10.1)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain) (2.9.2)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain) (8.5.0)\n",
      "Requirement already satisfied: ipython<9.0.0,>=8.10.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from genai) (8.29.0)\n",
      "Requirement already satisfied: openai<0.28.0,>=0.27.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from genai) (0.27.10)\n",
      "Collecting tabulate<0.10.0,>=0.9.0 (from genai)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Collecting tiktoken<0.4.0,>=0.3.2 (from genai)\n",
      "  Downloading tiktoken-0.3.3-cp311-cp311-win_amd64.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.17.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.23.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ipython<9.0.0,>=8.10.0->genai) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ipython<9.0.0,>=8.10.0->genai) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ipython<9.0.0,>=8.10.0->genai) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ipython<9.0.0,>=8.10.0->genai) (3.0.48)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ipython<9.0.0,>=8.10.0->genai) (2.18.0)\n",
      "Requirement already satisfied: stack-data in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ipython<9.0.0,>=8.10.0->genai) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ipython<9.0.0,>=8.10.0->genai) (5.14.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ipython<9.0.0,>=8.10.0->genai) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from ipython<9.0.0,>=8.10.0->genai) (0.4.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openai<0.28.0,>=0.27.0->genai) (4.64.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic<3,>=1->langchain) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2->langchain) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2->langchain) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tiktoken<0.4.0,>=0.3.2->genai) (2024.9.11)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jedi>=0.16->ipython<9.0.0,>=8.10.0->genai) (0.8.4)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (24.1)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython<9.0.0,>=8.10.0->genai) (0.2.13)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from stack-data->ipython<9.0.0,>=8.10.0->genai) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from stack-data->ipython<9.0.0,>=8.10.0->genai) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from stack-data->ipython<9.0.0,>=8.10.0->genai) (0.2.3)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from asttokens>=2.1.0->stack-data->ipython<9.0.0,>=8.10.0->genai) (1.16.0)\n",
      "Downloading genai-2.1.0-py3-none-any.whl (16 kB)\n",
      "Downloading langsmith-0.0.92-py3-none-any.whl (56 kB)\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Downloading tiktoken-0.3.3-cp311-cp311-win_amd64.whl (579 kB)\n",
      "   ---------------------------------------- 0.0/579.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 579.4/579.4 kB 5.0 MB/s eta 0:00:00\n",
      "Installing collected packages: tabulate, tiktoken, langsmith, genai\n",
      "  Attempting uninstall: tiktoken\n",
      "    Found existing installation: tiktoken 0.4.0\n",
      "    Uninstalling tiktoken-0.4.0:\n",
      "      Successfully uninstalled tiktoken-0.4.0\n",
      "  Attempting uninstall: langsmith\n",
      "    Found existing installation: langsmith 0.1.147\n",
      "    Uninstalling langsmith-0.1.147:\n",
      "      Successfully uninstalled langsmith-0.1.147\n",
      "Successfully installed genai-2.1.0 langsmith-0.0.92 tabulate-0.9.0 tiktoken-0.3.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-core 0.3.27 requires langsmith<0.3,>=0.1.125, but you have langsmith 0.0.92 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain genai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cold\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "response = model.generate_content(\"The opposite of hot is\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'BaseLLM' from 'langchain.schema' (c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\schema\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[86], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PromptTemplate\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LLMChain\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschema\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseLLM\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GenerativeModel\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Define a custom wrapper for the GenAI model\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'BaseLLM' from 'langchain.schema' (c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\schema\\__init__.py)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "london\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=['country'],\n",
    "    template=\"Tell me the capital of {country}.\"\n",
    ")\n",
    "# Use LLMChain with the initialized model\n",
    "chain = LLMChain(llm=hugging_face_hub, prompt=prompt_template)\n",
    "# Run the chain with input\n",
    "output = chain.run({'country': 'UK'})\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MULTIPLE CHAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captial_prompt = PromptTemplate(\n",
    "    input_variables=['country'],\n",
    "    template=\"Tell me the capital of {country}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captial_chain = LLMChain(llm= llm ,  prompt=captial_prompt )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "famous_prompt = PromptTemplate(\n",
    "    input_variables=['capital'],\n",
    "    template=\"suggest me some amazing places to visit in {captial}.\"\n",
    ")\n",
    "famous_chain =  LLMChain(llm= llm ,  prompt=famous_prompt )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The National Museum of India\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "from langchain.llms import HuggingFaceHub\n",
    "\n",
    "# Initialize the HuggingFaceHub LLM\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"google/flan-t5-large\", \n",
    "    model_kwargs={\"temperature\": 0, \"max_length\": 64}\n",
    ")\n",
    "\n",
    "# Define the prompt template for finding the capital\n",
    "capital_prompt = PromptTemplate(\n",
    "    input_variables=[\"country\"],\n",
    "    template=\"Tell me the capital of {country}.\"\n",
    ")\n",
    "\n",
    "# Create the first chain for finding the capital\n",
    "capital_chain = LLMChain(llm=llm, prompt=capital_prompt)\n",
    "\n",
    "# Define the prompt template for suggesting places\n",
    "famous_prompt = PromptTemplate(\n",
    "    input_variables=[\"capital\"],\n",
    "    template=\"Suggest me some amazing places to visit in {capital}.\"\n",
    ")\n",
    "\n",
    "# Create the second chain for suggesting places\n",
    "famous_chain = LLMChain(llm=llm, prompt=famous_prompt)\n",
    "\n",
    "# Combine both chains into a sequential chain\n",
    "chain = SimpleSequentialChain(chains=[capital_chain, famous_chain])\n",
    "\n",
    "# Run the chain with input\n",
    "output = chain.run(\"India\")\n",
    "\n",
    "# Print the output\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'New Delhi'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
    "\n",
    "prompt_template = \"Tell me a capital of {country} \"\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"country\"], template=prompt_template\n",
    ")\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "chain.invoke(\"india\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for LLMChain\nllm\n  Can't instantiate abstract class BaseLanguageModel with abstract methods agenerate_prompt, apredict, apredict_messages, generate_prompt, invoke, predict, predict_messages (type=type_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[101], line 22\u001b[0m\n\u001b[0;32m     16\u001b[0m capital_prompt \u001b[38;5;241m=\u001b[39m PromptTemplate(\n\u001b[0;32m     17\u001b[0m     input_variables\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcountry\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     18\u001b[0m     template\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTell me the capital of \u001b[39m\u001b[38;5;132;01m{country}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     19\u001b[0m )\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Create the first chain for finding the capital\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m capital_chain \u001b[38;5;241m=\u001b[39m \u001b[43mLLMChain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapital_prompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Define the prompt template for suggesting places\u001b[39;00m\n\u001b[0;32m     25\u001b[0m famous_prompt \u001b[38;5;241m=\u001b[39m PromptTemplate(\n\u001b[0;32m     26\u001b[0m     input_variables\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcapital\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     27\u001b[0m     template\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSuggest me some amazing places to visit in \u001b[39m\u001b[38;5;132;01m{capital}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     28\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\load\\serializable.py:75\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 75\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pydantic\\v1\\main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[0;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[1;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for LLMChain\nllm\n  Can't instantiate abstract class BaseLanguageModel with abstract methods agenerate_prompt, apredict, apredict_messages, generate_prompt, invoke, predict, predict_messages (type=type_error)"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Initialize the Google Gemini Pro model\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-pro\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "# Define the prompt template for finding the capital\n",
    "capital_prompt = PromptTemplate(\n",
    "    input_variables=[\"country\"],\n",
    "    template=\"Tell me the capital of {country}.\"\n",
    ")\n",
    "\n",
    "# Create the first chain for finding the capital\n",
    "capital_chain = LLMChain(llm=llm, prompt=capital_prompt)\n",
    "\n",
    "# Define the prompt template for suggesting places\n",
    "famous_prompt = PromptTemplate(\n",
    "    input_variables=[\"capital\"],\n",
    "    template=\"Suggest me some amazing places to visit in {capital}.\"\n",
    ")\n",
    "\n",
    "# Create the second chain for suggesting places\n",
    "famous_chain = LLMChain(llm=llm, prompt=famous_prompt)\n",
    "\n",
    "# Combine both chains into a sequential chain\n",
    "chain = SimpleSequentialChain(chains=[capital_chain, famous_chain])\n",
    "\n",
    "# Run the chain with input\n",
    "output = chain.run(\"India\")\n",
    "\n",
    "# Print the output\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['InputType',\n",
       " 'OutputType',\n",
       " '__abstractmethods__',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__class_getitem__',\n",
       " '__class_vars__',\n",
       " '__copy__',\n",
       " '__deepcopy__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__fields_set__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__get_pydantic_core_schema__',\n",
       " '__get_pydantic_json_schema__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__or__',\n",
       " '__orig_bases__',\n",
       " '__parameters__',\n",
       " '__pretty__',\n",
       " '__private_attributes__',\n",
       " '__pydantic_complete__',\n",
       " '__pydantic_core_schema__',\n",
       " '__pydantic_custom_init__',\n",
       " '__pydantic_decorators__',\n",
       " '__pydantic_extra__',\n",
       " '__pydantic_fields_set__',\n",
       " '__pydantic_generic_metadata__',\n",
       " '__pydantic_init_subclass__',\n",
       " '__pydantic_parent_namespace__',\n",
       " '__pydantic_post_init__',\n",
       " '__pydantic_private__',\n",
       " '__pydantic_root_model__',\n",
       " '__pydantic_serializer__',\n",
       " '__pydantic_validator__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__repr_args__',\n",
       " '__repr_name__',\n",
       " '__repr_str__',\n",
       " '__rich_repr__',\n",
       " '__ror__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__signature__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abatch_with_config',\n",
       " '_abc_impl',\n",
       " '_acall_with_config',\n",
       " '_agenerate',\n",
       " '_agenerate_with_cache',\n",
       " '_all_required_field_names',\n",
       " '_astream',\n",
       " '_atransform_stream_with_config',\n",
       " '_batch_with_config',\n",
       " '_calculate_keys',\n",
       " '_call_async',\n",
       " '_call_with_config',\n",
       " '_check_frozen',\n",
       " '_combine_llm_outputs',\n",
       " '_convert_input',\n",
       " '_copy_and_set_values',\n",
       " '_generate',\n",
       " '_generate_with_cache',\n",
       " '_get_invocation_params',\n",
       " '_get_llm_string',\n",
       " '_get_ls_params',\n",
       " '_get_value',\n",
       " '_identifying_params',\n",
       " '_is_protocol',\n",
       " '_iter',\n",
       " '_llm_type',\n",
       " '_model_family',\n",
       " '_prepare_params',\n",
       " '_prepare_request',\n",
       " '_serialized',\n",
       " '_should_stream',\n",
       " '_stream',\n",
       " '_supports_tool_choice',\n",
       " '_transform_stream_with_config',\n",
       " 'abatch',\n",
       " 'abatch_as_completed',\n",
       " 'agenerate',\n",
       " 'agenerate_prompt',\n",
       " 'ainvoke',\n",
       " 'apredict',\n",
       " 'apredict_messages',\n",
       " 'as_tool',\n",
       " 'assign',\n",
       " 'astream',\n",
       " 'astream_events',\n",
       " 'astream_log',\n",
       " 'async_client',\n",
       " 'atransform',\n",
       " 'batch',\n",
       " 'batch_as_completed',\n",
       " 'bind',\n",
       " 'bind_tools',\n",
       " 'call_as_llm',\n",
       " 'config_schema',\n",
       " 'config_specs',\n",
       " 'configurable_alternatives',\n",
       " 'configurable_fields',\n",
       " 'construct',\n",
       " 'copy',\n",
       " 'create_cached_content',\n",
       " 'dict',\n",
       " 'from_orm',\n",
       " 'generate',\n",
       " 'generate_prompt',\n",
       " 'get_config_jsonschema',\n",
       " 'get_graph',\n",
       " 'get_input_jsonschema',\n",
       " 'get_input_schema',\n",
       " 'get_lc_namespace',\n",
       " 'get_name',\n",
       " 'get_num_tokens',\n",
       " 'get_num_tokens_from_messages',\n",
       " 'get_output_jsonschema',\n",
       " 'get_output_schema',\n",
       " 'get_prompts',\n",
       " 'get_token_ids',\n",
       " 'input_schema',\n",
       " 'invoke',\n",
       " 'is_lc_serializable',\n",
       " 'json',\n",
       " 'lc_attributes',\n",
       " 'lc_id',\n",
       " 'lc_secrets',\n",
       " 'map',\n",
       " 'model_computed_fields',\n",
       " 'model_config',\n",
       " 'model_construct',\n",
       " 'model_copy',\n",
       " 'model_dump',\n",
       " 'model_dump_json',\n",
       " 'model_extra',\n",
       " 'model_fields',\n",
       " 'model_fields_set',\n",
       " 'model_json_schema',\n",
       " 'model_parametrized_name',\n",
       " 'model_post_init',\n",
       " 'model_rebuild',\n",
       " 'model_validate',\n",
       " 'model_validate_json',\n",
       " 'model_validate_strings',\n",
       " 'output_schema',\n",
       " 'parse_file',\n",
       " 'parse_obj',\n",
       " 'parse_raw',\n",
       " 'pick',\n",
       " 'pipe',\n",
       " 'predict',\n",
       " 'predict_messages',\n",
       " 'raise_deprecation',\n",
       " 'schema',\n",
       " 'schema_json',\n",
       " 'set_verbose',\n",
       " 'stream',\n",
       " 'to_json',\n",
       " 'to_json_not_implemented',\n",
       " 'transform',\n",
       " 'update_forward_refs',\n",
       " 'validate',\n",
       " 'validate_environment',\n",
       " 'with_alisteners',\n",
       " 'with_config',\n",
       " 'with_fallbacks',\n",
       " 'with_listeners',\n",
       " 'with_retry',\n",
       " 'with_structured_output',\n",
       " 'with_types']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(ChatGoogleGenerativeAI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
